{
  "llm": {
    "provider": "ollama",
    "connection": {
      "host": "localhost",
      "port": 11434,
      "timeout_seconds": 30
    },
    "model": {
      "name": "qwen3:0.6b",
      "fallback_models": ["qwen3:1.7b", "tinyllama"]
    },
    "inference": {
      "temperature": 0.3,
      "max_tokens": 512,
      "top_p": 0.9,
      "top_k": 40,
      "repeat_penalty": 1.1
    },
    "context": {
      "enabled": false,
      "max_history": 5
    }
  }
}
